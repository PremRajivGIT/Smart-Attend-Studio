import cv2
import os
import numpy as np
import albumentations as A
import random
from tqdm import tqdm
import shutil
import sys
from pathlib import Path
from ultralytics import YOLO
#from albumentations.augmentations.dropout.cutout import Cutout


def extract_frames(video_path, output_directory, class_name, num_frames=150, original_frames=20):
    """
    Extract frames from a video file

    Args:
        video_path: Path to the video file
        output_directory: Directory to save extracted frames
        class_name: Class name for the frames (derived from video filename)
        num_frames: Total number of frames to extract for augmentation
        original_frames: Number of original (unaugmented) frames to save

    Returns:
        List of paths to saved original frames and all frames
    """
    # Ensure output directory exists
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Open the video file
    cap = cv2.VideoCapture(video_path)

    # Check if video opened successfully
    if not cap.isOpened():
        print(f"Error: Could not open video file {video_path}")
        return [], []

    # Get video properties
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    print(f"Video has {frame_count} frames at {fps} FPS")

    # Determine frame indices to extract (evenly distributed)
    frame_indices = []
    if frame_count <= num_frames:
        frame_indices = list(range(frame_count))
    else:
        # Extract frames at regular intervals
        step = frame_count // num_frames
        frame_indices = [i * step for i in range(num_frames)]

    # Randomly select indices for original frames
    original_indices = random.sample(frame_indices, min(original_frames, len(frame_indices)))

    # Extract the frames
    saved_frames = []
    saved_original_frames = []

    print(f"Extracting {len(frame_indices)} frames from video...")

    for i, frame_idx in enumerate(tqdm(frame_indices)):
        # Set the frame position
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if not ret:
            print(f"Error reading frame {frame_idx}")
            continue

        # Save the frame with class name in the filename
        frame_path = os.path.join(output_directory, f"{class_name}_frame_{frame_idx:04d}.jpg")
        cv2.imwrite(frame_path, frame)
        saved_frames.append(frame_path)

        # If this is one of our original frames, save it to the list
        if frame_idx in original_indices:
            saved_original_frames.append(frame_path)

    # Release the video capture object
    cap.release()

    print(f"Extracted {len(saved_frames)} frames, including {len(saved_original_frames)} original frames")
    return saved_frames, saved_original_frames


import os
import cv2
import albumentations as A
from tqdm import tqdm

def apply_face_augmentations(image_paths, output_directory, num_augmentations=5):
    """
    Apply face-specific augmentations to images captured from plain background videos.
    Designed to maximize generalization while preventing multiple detections.

    Args:
        image_paths: List of paths to input images
        output_directory: Directory to save augmented images
        num_augmentations: Number of augmentations to apply per image

    Returns:
        List of paths to augmented images
    """
    # Ensure output directory exists
    os.makedirs(output_directory, exist_ok=True)

    # Base augmentation pipeline
 #   import albumentations as A

    base_transform = A.Compose([
        A.HorizontalFlip(p=0.5),
      #  A.Affine(scale=(0.9, 1.1), translate_percent=(0.05, 0.05), rotate=(-15, 15), p=0.5),
        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),
        A.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=5, p=0.5),
        A.ImageCompression(quality_range=(70,90),  p=0.3),
        A.GaussNoise(std_range=(0.05, 0.3), p=0.3),
        A.Rotate(limit=30,p=0.5),
        #A.ToGray(p=0.2),
        A.ColorJitter(brightness=0.2,contrast=0.2,saturation=0.2,hue=0.2,p=0.5),
    ])

    lighting_transform = A.Compose([
        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),
        A.RandomToneCurve(scale=0.1, p=0.7),
        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.5),
    ])

    environment_transform = A.Compose([
        A.RandomFog(fog_coef_range=(0.1, 0.3), alpha_coef=0.1, p=0.3),
        A.RandomRain(drop_length=3, drop_width=1, drop_color=(200, 200, 200), p=0.3),
        A.RandomSnow(snow_point_range=(0.1, 0.3), brightness_coeff=2.0, p=0.3),
    ])

    camera_transform = A.Compose([
        A.Downscale(scale_range=(0.5, 0.9), p=0.3),
        A.Blur(blur_limit=5, p=0.3),
        A.ImageCompression(quality_range=(50,80), p=0.5),
        A.MotionBlur(blur_limit=7, p=0.3),
    ])

    specialty_transforms = [lighting_transform, base_transform,environment_transform,camera_transform]
    augmented_image_paths = []

    print(f"Applying {num_augmentations} augmentations to {len(image_paths)} images...")

    for image_path in tqdm(image_paths):
        image = cv2.imread(image_path)

        if image is None:
            print(f"Error loading image {image_path}")
            continue

        base_name = os.path.basename(image_path)
        name_without_ext = os.path.splitext(base_name)[0]

        # Save original image with base transformation
        augmented = base_transform(image=image)
        augmented_image = augmented["image"]
        output_path = os.path.join(output_directory, f"{name_without_ext}_base.jpg")
        cv2.imwrite(output_path, augmented_image)
        augmented_image_paths.append(output_path)

        # Apply additional augmentations
        for i in range(num_augmentations - 1):
            specialty_transform = specialty_transforms[i % len(specialty_transforms)]

            augmented = base_transform(image=image)
            base_augmented = augmented["image"]
            augmented = specialty_transform(image=base_augmented)
            augmented_image = augmented["image"]

            output_path = os.path.join(output_directory, f"{name_without_ext}_aug{i + 1}.jpg")
            cv2.imwrite(output_path, augmented_image)
            augmented_image_paths.append(output_path)

    print(f"Created {len(augmented_image_paths)} augmented images")
    return augmented_image_paths


def generate_synthetic_backgrounds(output_directory, num_backgrounds=50, img_size=(640, 640)):
    """
    Generate synthetic backgrounds to add variety to the dataset.
    This can help the model distinguish faces from backgrounds better.

    Args:
        output_directory: Directory to save background images
        num_backgrounds: Number of synthetic backgrounds to generate
        img_size: Size of generated background images

    Returns:
        List of paths to generated background images
    """
    # Create the output directory if it doesn't exist
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Initialize a list to store paths to the generated backgrounds
    background_paths = []

    print(f"Generating {num_backgrounds} synthetic backgrounds...")

    # Loop to generate the specified number of backgrounds
    for i in range(num_backgrounds):
        # Create a blank image with the specified size and 3 color channels (RGB)
        background = np.zeros((img_size[0], img_size[1], 3), dtype=np.uint8)

        # Randomly choose a background type (removed 'pattern' to avoid shapes)
        bg_type = np.random.choice(['gradient', 'noise', 'solid'])

        if bg_type == 'gradient':
            # Create a gradient background
            direction = np.random.choice(['horizontal', 'vertical', 'diagonal'])
            color1 = np.random.randint(0, 256, 3).tolist()  # Random color 1
            color2 = np.random.randint(0, 256, 3).tolist()  # Random color 2

            if direction == 'horizontal':
                # Horizontal gradient: interpolate colors across the width
                for x in range(img_size[1]):
                    ratio = x / img_size[1]
                    color = [int(c1 * (1 - ratio) + c2 * ratio) for c1, c2 in zip(color1, color2)]
                    background[:, x] = color
            elif direction == 'vertical':
                # Vertical gradient: interpolate colors across the height
                for y in range(img_size[0]):
                    ratio = y / img_size[0]
                    color = [int(c1 * (1 - ratio) + c2 * ratio) for c1, c2 in zip(color1, color2)]
                    background[y, :] = color
            else:  # diagonal
                # Diagonal gradient: interpolate colors across both dimensions
                for y in range(img_size[0]):
                    for x in range(img_size[1]):
                        ratio = (x + y) / (img_size[0] + img_size[1])
                        color = [int(c1 * (1 - ratio) + c2 * ratio) for c1, c2 in zip(color1, color2)]
                        background[y, x] = color

        elif bg_type == 'noise':
            # Create a noise background
            noise_type = np.random.choice(['gaussian', 'salt_pepper', 'speckle'])

            if noise_type == 'gaussian':
                # Start with a solid color
                base_color = np.random.randint(100, 200, 3)
                background[:] = base_color

                # Add Gaussian noise
                noise = np.random.normal(0, 30, (img_size[0], img_size[1], 3))
                background = np.clip(background + noise, 0, 255).astype(np.uint8)

            elif noise_type == 'salt_pepper':
                # Start with a solid color
                base_color = np.random.randint(100, 200, 3)
                background[:] = base_color

                # Add salt and pepper noise
                density = 0.1
                mask = np.random.random((img_size[0], img_size[1])) < density
                background[mask] = [255, 255, 255]  # Salt (white)
                mask = np.random.random((img_size[0], img_size[1])) < density
                background[mask] = [0, 0, 0]  # Pepper (black)

            else:  # speckle
                # Create a base color
                base_color = np.random.randint(100, 200, 3).astype(np.float32)
                background[:] = base_color

                # Add speckle noise
                noise = np.random.randn(img_size[0], img_size[1], 3) * 30
                background = np.clip(background + background * noise * 0.1, 0, 255).astype(np.uint8)

        else:  # solid
            # Create a solid color background
            color = np.random.randint(0, 256, 3).tolist()
            background[:] = color

        # Apply some random transformations to make the background more realistic
        transform = A.Compose([
            A.GaussianBlur(blur_limit=(3, 7), p=0.5),
            A.GaussNoise(std_range=(0.05, 0.3), p=0.5),
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
        ])

        # Apply the transformations to the background
        background = transform(image=background)["image"]

        # Save the background image
        output_path = os.path.join(output_directory, f"synthetic_bg_{i + 1}.jpg")
        cv2.imwrite(output_path, background)
        background_paths.append(output_path)

    print(f"Generated {len(background_paths)} synthetic backgrounds")
    return background_paths

def blend_face_with_background(face_image, background_image, alpha_range=(0.8, 1.0)):
    """
    Blend a face image with a background image using alpha blending.
    Helps create more diverse training images.

    Args:
        face_image: Face image to blend
        background_image: Background image to blend with
        alpha_range: Range of alpha values for blending (higher = more face visibility)

    Returns:
        Blended image
    """
    # Resize background to match face image if needed
    if face_image.shape[:2] != background_image.shape[:2]:
        background_image = cv2.resize(background_image, (face_image.shape[1], face_image.shape[0]))

    # Randomly select alpha value from range
    alpha = np.random.uniform(alpha_range[0], alpha_range[1])

    # Blend images
    blended = cv2.addWeighted(face_image, alpha, background_image, 1 - alpha, 0)

    return blended


def detect_faces_with_yolov8(image_path, output_annotation_path, model, class_id=0, conf_threshold=0.25):
    """
    Detect faces using YOLOv8 and generate YOLO annotations

    Args:
        image_path: Path to the input image
        output_annotation_path: Path to save the annotation file
        model: YOLOv8 model for detection
        class_id: Class ID for the annotation (derived from video filename)
        conf_threshold: Confidence threshold for detections

    Returns:
        Boolean indicating success and number of faces detected
    """
    # Load the image
    image = cv2.imread(image_path)

    if image is None:
        print(f"Error loading image {image_path}")
        return False, 0

    # Get image dimensions
    height, width = image.shape[:2]

    # Run YOLOv8 detection
    results = model(image)

    # Check if any faces were detected
    boxes = []
    for result in results:
        for box in result.boxes:
            # For face detection model, class 0 is face
            # For general models, class 0 is person, but we'll use it as fallback
            if box.conf[0] >= conf_threshold:
                # Extract coordinates
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()

                # Convert to YOLO format (center_x, center_y, width, height) - normalized
                center_x = ((x1 + x2) / 2) / width
                center_y = ((y1 + y2) / 2) / height
                box_width = (x2 - x1) / width
                box_height = (y2 - y1) / height

                # Ensure values are in range [0, 1]
                center_x = max(0, min(1, center_x))
                center_y = max(0, min(1, center_y))
                box_width = max(0, min(1, box_width))
                box_height = max(0, min(1, box_height))

                boxes.append((center_x, center_y, box_width, box_height))

    # If no faces detected using YOLO, use a center region fallback
    if len(boxes) == 0:
        # Use a region in the center of the image
        center_x = 0.5
        center_y = 0.5
        # Use a reasonable size for a face (40% of image width and height)
        box_width = 0.4
        box_height = 0.4
        boxes.append((center_x, center_y, box_width, box_height))
        print(f"No faces detected in {image_path}, using center region fallback")

    # Create the annotation file
    with open(output_annotation_path, "w") as f:
        for box in boxes:
            center_x, center_y, box_width, box_height = box
            f.write(f"{class_id} {center_x} {center_y} {box_width} {box_height}\n")

    return True, len(boxes)


def validate_detection_accuracy(image_path, output_path, annotation_path=None, class_names=None):
    """
    Utility function to validate detection by drawing the box on the image

    Args:
        image_path: Path to the input image
        output_path: Path to save the validation image
        annotation_path: Path to the annotation file
        class_names: Dictionary mapping class IDs to class names

    Returns:
        Boolean indicating success
    """
    # Load the image
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error loading image {image_path}")
        return False

    height, width = image.shape[:2]

    # If an annotation file is provided, use it
    if annotation_path and os.path.exists(annotation_path):
        with open(annotation_path, 'r') as f:
            lines = f.readlines()
            for line in lines:
                parts = line.strip().split()
                if len(parts) >= 5:
                    class_id, center_x, center_y, box_w, box_h = parts
                    class_id = int(class_id)
                    center_x, center_y, box_w, box_h = map(float, [center_x, center_y, box_w, box_h])

                    # Convert from YOLO format to pixel coordinates
                    center_x = center_x * width
                    center_y = center_y * height
                    box_w = box_w * width
                    box_h = box_h * height

                    # Calculate top-left coordinates from center
                    x = int(center_x - (box_w / 2))
                    y = int(center_y - (box_h / 2))
                    w = int(box_w)
                    h = int(box_h)

                    # Draw rectangle
                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

                    # Get class name if available
                    if class_names and class_id in class_names:
                        class_label = class_names[class_id]
                    else:
                        class_label = f"Class {class_id}"

                    cv2.putText(image, class_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Save the visualization
    cv2.imwrite(output_path, image)
    return True


def process_video(video_path, output_base_dir, class_id, class_name,
                           num_frames=200, original_frames=30, augmentations_per_frame=5,
                           yolo_model=None, generate_backgrounds=True):
    """
    Enhanced video processing workflow specifically for face detection from plain background videos.
    Includes improved frame extraction, augmentation, and annotation strategies.

    Args:
        video_path: Path to the input video
        output_base_dir: Base directory for all outputs
        class_id: Numeric ID for the class (used in YOLO labels)
        class_name: String name for the class (derived from video filename)
        num_frames: Total number of frames to extract
        original_frames: Number of original frames to save
        augmentations_per_frame: Number of augmentations to apply per frame
        yolo_model: Pre-loaded YOLO model for face detection
        generate_backgrounds: Whether to generate synthetic backgrounds

    Returns:
        Tuple containing dataset directories
    """
    # Create output directories
    frames_dir = os.path.join(output_base_dir, "extracted_frames", class_name)
    augmented_dir = os.path.join(output_base_dir, "augmented_frames", class_name)
    backgrounds_dir = os.path.join(output_base_dir, "synthetic_backgrounds")
    dataset_dir = os.path.join(output_base_dir, "dataset")
    images_dir = os.path.join(dataset_dir, "images")
    labels_dir = os.path.join(dataset_dir, "labels")
    validation_dir = os.path.join(output_base_dir, "validation_images", class_name)

    for directory in [frames_dir, augmented_dir, backgrounds_dir, dataset_dir,
                      images_dir, labels_dir, validation_dir]:
        if not os.path.exists(directory):
            os.makedirs(directory)

    # Step 1: Extract frames from video with improved frame selection
    print(f"\nStep 1: Extracting frames from {video_path} as class '{class_name}' (ID: {class_id})")

    # Open the video file
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {video_path}")
        return None, None, None

    # Get video properties
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"Video has {frame_count} frames at {fps} FPS")

    # Extract frames with improved strategy - capture more diverse angles
    saved_frames = []

    # Method 1: Evenly distributed frames for diversity
    if frame_count <= num_frames:
        frame_indices = list(range(frame_count))
    else:
        # Extract frames at regular intervals
        step = frame_count // (num_frames // 2)
        regular_indices = [i * step for i in range(num_frames // 2)]

        # Also extract frames with motion detection for additional diversity
        prev_frame = None
        motion_indices = []
        frame_diffs = []

        print("Analyzing video for frames with significant motion...")
        for i in tqdm(range(0, frame_count, max(1, frame_count // 100))):  # Sample frames for efficiency
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                continue

            if prev_frame is not None:
                # Convert to grayscale for motion detection
                gray1 = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
                gray2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

                # Calculate absolute difference
                diff = cv2.absdiff(gray1, gray2)
                mean_diff = np.mean(diff)
                frame_diffs.append((i, mean_diff))

            prev_frame = frame.copy()

        # Sort by difference and get indices with highest motion
        frame_diffs.sort(key=lambda x: x[1], reverse=True)
        motion_indices = [idx for idx, _ in frame_diffs[:num_frames // 2]]

        # Combine and eliminate duplicates
        frame_indices = sorted(list(set(regular_indices + motion_indices)))[:num_frames]

    print(f"Selected {len(frame_indices)} frames to extract...")

    # Extract the selected frames
    for frame_idx in tqdm(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if not ret:
            print(f"Error reading frame {frame_idx}")
            continue

        # Save the frame with class name in the filename
        frame_path = os.path.join(frames_dir, f"{class_name}_frame_{frame_idx:04d}.jpg")
        cv2.imwrite(frame_path, frame)
        saved_frames.append(frame_path)

    cap.release()
    print(f"Extracted {len(saved_frames)} frames")

    # Step 2: Generate synthetic backgrounds if needed
    backgrounds = []
    if generate_backgrounds:
        print("\nStep 2: Generating synthetic backgrounds for augmentation diversity")
        backgrounds = generate_synthetic_backgrounds(backgrounds_dir, num_backgrounds=20)

    # Step 3: Apply face-specific augmentations
    print(f"\nStep 3: Applying augmentations to {len(saved_frames)} frames")
    augmented_paths = apply_face_augmentations(saved_frames, augmented_dir, augmentations_per_frame)

    # Step 4: Create additional blended background examples
    blended_paths = []
    if backgrounds and len(backgrounds) > 0:
        print("\nStep 4: Creating blended background examples")
        for i, face_path in enumerate(tqdm(random.sample(saved_frames, min(20, len(saved_frames))))):
            face_img = cv2.imread(face_path)
            if face_img is None:
                continue

            # Select a random background
            bg_path = random.choice(backgrounds)
            bg_img = cv2.imread(bg_path)
            if bg_img is None:
                continue

            # Blend face with background
            blended = blend_face_with_background(face_img, bg_img)

            # Save blended image
            base_name = os.path.basename(face_path)
            name_without_ext = os.path.splitext(base_name)[0]
            blended_path = os.path.join(augmented_dir, f"{name_without_ext}_blended_{i + 1}.jpg")
            cv2.imwrite(blended_path, blended)
            blended_paths.append(blended_path)

    # Combine all images
    all_images = saved_frames + augmented_paths + blended_paths
    random.shuffle(all_images)  # Shuffle to ensure random distribution

    # Load YOLOv8 model if not provided
    if yolo_model is None:
        print("\nLoading YOLOv8 model...")
        try:
            # Try loading the YOLOv8 model for face detection
            try:
                yolo_model = YOLO("yolov8n-face.pt")
                print("Loaded YOLOv8 face detection model")
            except:
                # Fallback to the standard YOLOv8 model
                yolo_model = YOLO("yolov8n.pt")
                print("Using standard YOLOv8 model (not optimized for face detection)")
        except Exception as e:
            print(f"Error loading YOLOv8 model: {e}")
            return None, None, None

    # Step 5: Generate annotations for all images
    print(f"\nStep 5: Generating annotations for {len(all_images)} images with class ID {class_id}")

    success_count = 0
    face_detected_count = 0
    fallback_count = 0
    processed_image_paths = []

    for image_path in tqdm(all_images):
        base_name = os.path.basename(image_path)
        base_name_without_ext = os.path.splitext(base_name)[0]

        # Copy image to dataset directory
        dataset_image_path = os.path.join(images_dir, base_name)
        shutil.copy(image_path, dataset_image_path)

        # Create annotation file
        annotation_file_name = f"{base_name_without_ext}.txt"
        output_annotation_path = os.path.join(labels_dir, annotation_file_name)

        # Generate YOLO annotations with the specific class ID
        # Use improved face detection that prevents multiple detections
        image = cv2.imread(image_path)
        if image is None:
            print(f"Error loading image {image_path}")
            continue

        # Get image dimensions
        height, width = image.shape[:2]

        # Run YOLOv8 detection
        results = yolo_model(image)

        # Check if any faces were detected
        boxes = []
        confidences = []

        for result in results:
            for box in result.boxes:
                if box.conf[0] >= 0.25:  # Confidence threshold
                    # Extract coordinates
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0])

                    # Convert to YOLO format (center_x, center_y, width, height) - normalized
                    center_x = ((x1 + x2) / 2) / width
                    center_y = ((y1 + y2) / 2) / height
                    box_width = (x2 - x1) / width
                    box_height = (y2 - y1) / height

                    # Ensure values are in range [0, 1]
                    center_x = max(0, min(1, center_x))
                    center_y = max(0, min(1, center_y))
                    box_width = max(0, min(1, box_width))
                    box_height = max(0, min(1, box_height))

                    boxes.append((center_x, center_y, box_width, box_height))
                    confidences.append(conf)

        # For face detection training, we want ONE box per face to prevent multiple detections
        if len(boxes) > 1:
            # Keep only the most confident detection for training
            best_idx = confidences.index(max(confidences))
            boxes = [boxes[best_idx]]

        # If no faces detected, use a center region fallback
        if len(boxes) == 0:
            # Use a region in the center of the image
            center_x, center_y = 0.5, 0.5
            # Use a reasonable size for a face (35% of image width and height)
            box_width, box_height = 0.35, 0.35
            boxes.append((center_x, center_y, box_width, box_height))
            fallback_count += 1
        else:
            face_detected_count += 1

        # Create the annotation file
        with open(output_annotation_path, "w") as f:
            for box in boxes:
                center_x, center_y, box_width, box_height = box
                f.write(f"{class_id} {center_x} {center_y} {box_width} {box_height}\n")

        success_count += 1

        # Create validation image with bounding box
        validation_image_path = os.path.join(validation_dir, f"validated_{base_name}")

        # Draw the bounding box on the image
        for center_x, center_y, box_width, box_height in boxes:
            # Convert to pixel coordinates
            x = int((center_x - box_width / 2) * width)
            y = int((center_y - box_height / 2) * height)
            w = int(box_width * width)
            h = int(box_height * height)

            # Draw rectangle
            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

            # Add class name
            cv2.putText(image, class_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        # Save validation image
        cv2.imwrite(validation_image_path, image)
        processed_image_paths.append(validation_image_path)

    # Print summary
    print(f"\nProcessing Complete for {class_name}!")
    print(f"Total images processed: {len(all_images)}")
    print(f"Successful processing: {success_count}")
    print(f"Face detection successful: {face_detected_count}")
    print(f"Fallback to center region: {fallback_count}")

    return dataset_dir, images_dir, labels_dir


def process_videos_directory(input_directory, output_directory, yolo_model=None):
    """
    Process all videos in the input directory

    Args:
        input_directory: Path to directory containing videos
        output_directory: Path to output directory
        yolo_model: Pre-loaded YOLO model for face detection

    Returns:
        Path to dataset directory, list of class names, and class mapping
    """
    # Create output directory structure
    dataset_dir = os.path.join(output_directory, "dataset")
    images_dir = os.path.join(dataset_dir, "images")
    labels_dir = os.path.join(dataset_dir, "labels")

    for directory in [dataset_dir, images_dir, labels_dir]:
        if not os.path.exists(directory):
            os.makedirs(directory)

    # Load YOLOv8 model if not provided
    if yolo_model is None:
        print("\nLoading YOLOv8 model...")
        try:
            # Try loading the YOLOv8 model for face detection
            try:
                yolo_model = YOLO("yolov8n-face.pt")
                print("Loaded YOLOv8 face detection model")
            except:
                # Fallback to the standard YOLOv8 model
                yolo_model = YOLO("yolov8n.pt")
                print("Using standard YOLOv8 model (not optimized for face detection)")
        except Exception as e:
            print(f"Error loading YOLOv8 model: {e}")
            return None, [], {}

    # Find all videos in the input directory
    video_paths = []
    for filename in os.listdir(input_directory):
        if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
            video_paths.append(os.path.join(input_directory, filename))

    # Exit if no videos found
    if not video_paths:
        print(f"No video files found in {input_directory}")
        return None, [], {}

    print(f"\nFound {len(video_paths)} videos to process")

    # Process each video and assign class IDs by video name
    class_names = []
    class_mapping = {}

    for video_idx, video_path in enumerate(video_paths):
        # Extract class name from video file name (without extension)
        video_filename = os.path.basename(video_path)
        class_name = os.path.splitext(video_filename)[0]
        class_id = video_idx

        # Add to class lists
        class_names.append(class_name)
        class_mapping[class_id] = class_name

        # Process the video
        print(f"\nProcessing video {video_idx + 1}/{len(video_paths)}: {video_filename}")
        dataset_dir, images_dir, labels_dir = process_video(
            video_path=video_path,
            output_base_dir=output_directory,
            class_id=class_id,
            class_name=class_name,
            num_frames=60,  # Number of frames to extract
            original_frames=20,  # Number of original frames to keep
            augmentations_per_frame=3,  # Number of augmentations per frame
            yolo_model=yolo_model
        )

    print(f"\nAll videos processed. Created dataset with {len(class_names)} classes:")
    for class_id, class_name in class_mapping.items():
        print(f"  Class {class_id}: {class_name}")

    return dataset_dir, class_names, class_mapping


def split_train_val(dataset_dir, train_percent=0.8):
    """
    Split the dataset into training and validation sets

    Args:
        dataset_dir: Path to the dataset directory containing images and labels
        train_percent: Percentage of images for training (0.01-0.99)

    Returns:
        Path to data directory containing train and validation splits
    """
    # Define paths to image and annotation folders
    input_image_path = os.path.join(dataset_dir, 'images')
    input_label_path = os.path.join(dataset_dir, 'labels')

    # Define paths for train and validation folders
    cwd = os.getcwd()
    data_dir = os.path.join(cwd, 'data')
    train_img_path = os.path.join(data_dir, 'train/images')
    train_txt_path = os.path.join(data_dir, 'train/labels')
    val_img_path = os.path.join(data_dir, 'validation/images')
    val_txt_path = os.path.join(data_dir, 'validation/labels')

    # Create folders if they don't already exist
    for dir_path in [train_img_path, train_txt_path, val_img_path, val_txt_path]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)

    # Get list of all images
    img_file_list = [path for path in Path(input_image_path).rglob('*') if
                     path.suffix.lower() in ['.jpg', '.jpeg', '.png']]

    print(f'Number of image files: {len(img_file_list)}')

    # Group images by class for stratified splitting
    class_images = {}
    for img_path in img_file_list:
        # Extract class from filename (assuming format: classname_frame_xxxx.jpg)
        filename = img_path.name
        class_name = filename.split('_')[0]

        if class_name not in class_images:
            class_images[class_name] = []

        class_images[class_name].append(img_path)

    print(f"Found {len(class_images)} classes in the dataset")

    # Move images to train/val while maintaining class balance
    train_count = 0
    val_count = 0

    for class_name, images in class_images.items():
        print(f"Class '{class_name}': {len(images)} images")

        # Calculate number of training images for this class
        train_size = int(len(images) * train_percent)

        # Shuffle images
        random.shuffle(images)

        # Split into train/val
        train_images = images[:train_size]
        val_images = images[train_size:]

        # Copy train images
        for img_path in train_images:
            img_fn = img_path.name
            base_fn = img_path.stem
            txt_fn = base_fn + '.txt'
            txt_path = os.path.join(input_label_path, txt_fn)

            shutil.copy(img_path, os.path.join(train_img_path, img_fn))
            if os.path.exists(txt_path):
                shutil.copy(txt_path, os.path.join(train_txt_path, txt_fn))

            train_count += 1

        # Copy validation images
        for img_path in val_images:
            img_fn = img_path.name
            base_fn = img_path.stem
            txt_fn = base_fn + '.txt'
            txt_path = os.path.join(input_label_path, txt_fn)

            shutil.copy(img_path, os.path.join(val_img_path, img_fn))
            if os.path.exists(txt_path):
                shutil.copy(txt_path, os.path.join(val_txt_path, txt_fn))

            val_count += 1

    print(f"\nSplitting complete!")
    print(f"Training images: {train_count}")
    print(f"Validation images: {val_count}")

    return data_dir


def create_yaml_file(data_dir, class_names):
    """
    Create a YAML configuration file for YOLOv5/YOLOv8 training

    Args:
        data_dir: Path to the data directory
        class_names: List of class names

    Returns:
        Path to the created YAML file
    """
    # Create relative paths for YAML file (to make it more portable)
    train_path = './train'
    val_path = './validation'

    # Format class names for YAML
    names_str = ", ".join([f"'{name}'" for name in class_names])

    # Create YAML content
    yaml_content = f"""# YOLOv5/YOLOv8 dataset configuration
# Path to datasets
train: {train_path}
val: {val_path}

# Classes
nc: {len(class_names)}  # number of classes
names: [{names_str}]  # class names

# Training parameters
# Uncomment and modify these as needed for your training
# batch: 16
# epochs: 100
# img_size: [640, 640]
# patience: 50
"""

    # Write YAML file
    yaml_path = os.path.join(data_dir, "dataset.yaml")
    with open(yaml_path, 'w') as f:
        f.write(yaml_content)

    print(f"\nYAML configuration file created at: {yaml_path}")
    print("You can use this YAML file for YOLO training with commands like:")
    print(f"  yolo train model=yolov8n.pt data={yaml_path}")

    return yaml_path


def main():
    """Main function to run the multi-class face dataset creator pipeline"""
    # Define default parameters
    videos_folder = "C:/Users/premr/OneDrive/Desktop/NMC_FACE_VIDEOS"  # Replace with your videos folder
    output_path = "C:/Users/premr/OneDrive/Desktop/NMC_Input/Output"  # Default output directory
    train_percent = 0.8  # Percentage of images for training (0.01-0.99)

    # Allow command line arguments (optional)
    if len(sys.argv) > 1:
        videos_folder = sys.argv[1]
    if len(sys.argv) > 2:
        output_path = sys.argv[2]

    # Validate input path
    if not os.path.exists(videos_folder):
        print(f'Videos folder not found: {videos_folder}')
        sys.exit(1)

    # Create output directory
    if not os.path.exists(output_path):
        os.makedirs(output_path)

    # Load YOLO model
    print("Loading YOLOv8 model...")
    try:
        yolo_model = YOLO("yoloface.pt")
        print("Loaded YOLOv8 face detection model")
    except Exception as e:
        print(f"Error loading YOLOv8 model: {e}")
        sys.exit(1)

    # Process all videos in the directory
    print(f"\nProcessing all videos in: {videos_folder}")
    dataset_dir, class_names, class_mapping = process_videos_directory(
        input_directory=videos_folder,
        output_directory=output_path,
        yolo_model=yolo_model
    )

    if not dataset_dir:
        print("Error processing videos.")
        sys.exit(1)

    # Split the dataset into train and validation sets
    print("\nSplitting dataset into training and validation sets...")
    data_dir = split_train_val(dataset_dir, train_percent)

    # Create YAML configuration file with all class names
    print("\nCreating YAML configuration file...")
    yaml_path = create_yaml_file(data_dir, class_names)

    print("\nAll processing complete!")
    print(f"Your dataset is ready for YOLO training with {len(class_names)} classes:")
    for i, class_name in enumerate(class_names):
        print(f"  Class {i}: {class_name}")
    print(f"\nUse the YAML file at {yaml_path} for training.")


if __name__ == "__main__":
    main()
